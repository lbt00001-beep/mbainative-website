// Data: 10 AI Doctrines with thesis, proponents, and objections

export interface Doctrine {
    id: number;
    title: string;
    shortTitle: string;
    thesis: string;
    proponents: string[]; // Guru IDs
    objection: string;
    sources: { title: string; url: string }[];
    icon: string;
}

export const DOCTRINES: Doctrine[] = [
    {
        id: 1,
        title: "El escalado funcionaâ€¦ hasta que deje de hacerlo",
        shortTitle: "Scaling Laws",
        thesis: "MÃ¡s datos + mÃ¡s cÃ³mputo + mejores recetas â‡’ saltos de capacidad. El camino a AGI es escalar modelos cada vez mÃ¡s grandes.",
        proponents: ["sam-altman", "demis-hassabis", "jensen-huang", "andrej-karpathy"],
        objection: "Los lÃ­mites del 'solo escalar' estÃ¡n siendo cuestionados. Â¿Hay un techo? El debate sigue abierto en 2025.",
        sources: [
            { title: "OpenAI Scaling Laws", url: "https://openai.com/" },
            { title: "DeepMind Research", url: "https://deepmind.google/" }
        ],
        icon: "ğŸ“ˆ"
    },
    {
        id: 2,
        title: "La IA es infraestructura: 'AI Factories'",
        shortTitle: "AI Factories",
        thesis: "El cuello de botella decisivo es energÃ­a, centros de datos, GPUs/redes. La ventaja competitiva es industrial, no algorÃ­tmica.",
        proponents: ["jensen-huang", "mustafa-suleyman"],
        objection: "ConcentraciÃ³n de poder, dependencia de supply chain, riesgos sistÃ©micos y barreras de entrada para nuevos actores.",
        sources: [
            { title: "NVIDIA AI Factories", url: "https://www.nvidia.com/" }
        ],
        icon: "ğŸ­"
    },
    {
        id: 3,
        title: "La prÃ³xima frontera: modelar el mundo (JEPA)",
        shortTitle: "World Models",
        thesis: "Para razonamiento robusto hace falta aprender modelos internos predictivos (JEPA / world models), no solo correlaciones lingÃ¼Ã­sticas.",
        proponents: ["yann-lecun"],
        objection: "AÃºn falta demostrar que estos enfoques superen al LLM+herramientas en tareas generales.",
        sources: [
            { title: "JEPA Paper", url: "https://openreview.net/" }
        ],
        icon: "ğŸŒ"
    },
    {
        id: 4,
        title: "El problema central es el control",
        shortTitle: "Control Problem",
        thesis: "El esquema clÃ¡sico 'maximiza un objetivo fijo' es peligroso. Hay que diseÃ±ar agentes que no impongan objetivos mal especificados.",
        proponents: [], // Stuart Russell no estÃ¡ en la lista de gurÃºs
        objection: "Traducir esto a ingenierÃ­a concreta a escala (producto + economÃ­a) no es trivial.",
        sources: [
            { title: "Stuart Russell - Human Compatible", url: "https://people.eecs.berkeley.edu/~russell/" }
        ],
        icon: "ğŸ›ï¸"
    },
    {
        id: 5,
        title: "Alineamiento por principios: Constitutional AI",
        shortTitle: "Constitutional AI",
        thesis: "En vez de etiquetar cada caso con humanos, entrenas con una 'constituciÃ³n' de principios para auto-criticar y corregir.",
        proponents: ["dario-amodei"],
        objection: "Â¿QuiÃ©n escribe la 'constituciÃ³n'? Â¿CÃ³mo se audita? Â¿Y quÃ© pasa con valores en conflicto?",
        sources: [
            { title: "Constitutional AI Paper", url: "https://arxiv.org/abs/2212.08073" }
        ],
        icon: "ğŸ“œ"
    },
    {
        id: 6,
        title: "Safety-first como misiÃ³n Ãºnica",
        shortTitle: "Safety-First Labs",
        thesis: "Separar el incentivo comercial: construir superinteligencia solo si es segura. Laboratorios monotemÃ¡ticos en safety.",
        proponents: ["ilya-sutskever"],
        objection: "DifÃ­cil sostenerlo si el ecosistema premia velocidad/mercado. AdemÃ¡s 'seguro' es un criterio disputado.",
        sources: [
            { title: "Safe Superintelligence Inc.", url: "https://ssi.inc/" }
        ],
        icon: "ğŸ›¡ï¸"
    },
    {
        id: 7,
        title: "La doctrina de la alerta (desde dentro)",
        shortTitle: "Inside Warning",
        thesis: "La propia comunidad 'fundacional' legitima pÃºblicamente riesgos serios: misinfo, usos maliciosos, pÃ©rdida de control.",
        proponents: ["geoffrey-hinton", "yoshua-bengio"],
        objection: "Riesgo de sobrerreacciÃ³n regulatoria o de 'captura' del discurso por actores con agenda propia.",
        sources: [
            { title: "Hinton en The Guardian", url: "https://www.theguardian.com/" },
            { title: "Bengio AI Safety", url: "https://yoshuabengio.org/" }
        ],
        icon: "âš ï¸"
    },
    {
        id: 8,
        title: "IA centrada en la condiciÃ³n humana",
        shortTitle: "Human-Centered AI",
        thesis: "El norte es impacto humano: educaciÃ³n, polÃ­tica, Ã©tica, transparencia. DiseÃ±o para complementar capacidades humanas, no reemplazarlas.",
        proponents: ["fei-fei-li", "andrew-ng"],
        objection: "Puede quedarse en 'principios' si no aterriza en mÃ©tricas y mecanismos de enforcement.",
        sources: [
            { title: "Stanford HAI", url: "https://hai.stanford.edu/" }
        ],
        icon: "ğŸ¤"
    },
    {
        id: 9,
        title: "IA para ciencia",
        shortTitle: "AI for Science",
        thesis: "El valor mÃ¡s transformador es acelerar descubrimiento cientÃ­fico: proteÃ­nas, fÃ¡rmacos, materiales.",
        proponents: ["demis-hassabis"],
        objection: "Resultados espectaculares pero no generalizan automÃ¡ticamente. Exige validaciÃ³n experimental y gobernanza de acceso.",
        sources: [
            { title: "AlphaFold en Nature", url: "https://www.nature.com/" }
        ],
        icon: "ğŸ”¬"
    },
    {
        id: 10,
        title: "IA como sistema extractivo",
        shortTitle: "Extractive System",
        thesis: "La IA depende de extracciÃ³n de datos, trabajo, energÃ­a. Genera sesgos, opacidad y asimetrÃ­as de poder.",
        proponents: ["karen-hao"],
        objection: "Si se formula solo como crÃ­tica puede aportar poco a la ingenierÃ­a. Su fuerza estÃ¡ en auditorÃ­a y accountability.",
        sources: [
            { title: "Stochastic Parrots Paper", url: "https://dl.acm.org/doi/10.1145/3442188.3445922" }
        ],
        icon: "âš–ï¸"
    }
];

// Get doctrine by ID
export const getDoctrineById = (id: number): Doctrine | undefined => {
    return DOCTRINES.find(d => d.id === id);
};
